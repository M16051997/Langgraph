{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea4203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76dd7a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's delve into the chemical structure and properties of hydrogen.\n",
      "\n",
      "**Chemical Structure of Hydrogen**\n",
      "\n",
      "Hydrogen (symbol H) is the simplest and most abundant element in the universe. Its structure is incredibly straightforward:\n",
      "\n",
      "*   **Atomic Number:** 1\n",
      "*   **Number of Protons:** 1 (in the nucleus)\n",
      "*   **Number of Neutrons:** 0 (in the most common isotope, protium, ¹H)  Deuterium (²H) has 1 neutron, and Tritium (³H) has 2 neutrons.\n",
      "*   **Number of Electrons:** 1 (orbiting the nucleus)\n",
      "*   **Electron Configuration:** 1s¹\n",
      "\n",
      "**Key Points about the Structure:**\n",
      "\n",
      "*   **Simplicity:** Hydrogen's simplicity is its defining feature. It's just one proton and one electron.\n",
      "*   **Electron Placement:** That single electron resides in the 1s orbital, which is the lowest energy level available.\n",
      "*   **Isotopes:**  While the most common form has no neutrons, the presence of isotopes like deuterium and tritium influences some physical properties (like mass) and can be significant in nuclear reactions.\n",
      "*   **Diatomic Molecule (H₂):**  Elemental hydrogen *almost always* exists as a diatomic molecule, H₂.  Two hydrogen atoms share their electrons to form a covalent bond.  This is much more stable than a single hydrogen atom on its own.\n",
      "\n",
      "**Properties of Hydrogen**\n",
      "\n",
      "Hydrogen's properties are a direct consequence of its simple structure and its tendency to form a diatomic molecule:\n",
      "\n",
      "**Physical Properties:**\n",
      "\n",
      "*   **State at Room Temperature:** Gas\n",
      "*   **Color:** Colorless\n",
      "*   **Odor:** Odorless\n",
      "*   **Taste:** Tasteless\n",
      "*   **Density:** Very low (much lighter than air) - This is why hydrogen-filled balloons float.\n",
      "*   **Melting Point:** -259.14 °C (-434.45 °F)\n",
      "*   **Boiling Point:** -252.87 °C (-423.17 °F)\n",
      "*   **Thermal Conductivity:** Relatively high for a gas.  This means it can transfer heat effectively.\n",
      "*   **Solubility:** Slightly soluble in water, but more soluble in some organic solvents.\n",
      "*   **Flammability:** Highly flammable. It burns readily in the presence of oxygen.\n",
      "*   **Molecular Weight:** 1.008 g/mol (for ¹H) 2.016 g/mol (for H₂)\n",
      "\n",
      "**Chemical Properties:**\n",
      "\n",
      "*   **Reactivity:**\n",
      "    *   While H₂ is relatively unreactive at room temperature, it becomes much more reactive at elevated temperatures or in the presence of catalysts. The strong H-H bond needs to be broken, which requires energy.\n",
      "    *   It can react with a wide variety of elements, including oxygen, halogens, and some metals.\n",
      "*   **Oxidation State:** Can exist in oxidation states of +1 (e.g., in water, H₂O) or -1 (e.g., in metal hydrides, NaH).\n",
      "*   **Bonding:** Forms covalent bonds with nonmetals and ionic bonds with highly electropositive metals.\n",
      "*   **Reducing Agent:** It is a good reducing agent, meaning it can donate electrons to other substances.  This is used in many industrial processes.\n",
      "*   **Combustion:** Burns with a nearly invisible blue flame (in pure oxygen), producing water as the only product:\n",
      "    2H₂ (g) + O₂ (g) → 2H₂O (g) + Heat\n",
      "    This reaction is highly exothermic (releases a lot of heat).\n",
      "*   **Formation of Hydrides:** Reacts with many metals to form hydrides (compounds containing hydrogen bonded to a metal). These hydrides can be ionic (like NaH) or covalent (like TiH₂).\n",
      "*   **Isotopes:**  Deuterium and tritium have different reaction rates than protium (the most common isotope) due to the kinetic isotope effect (the heavier isotopes react slower).\n",
      "\n",
      "**Important Considerations/Applications:**\n",
      "\n",
      "*   **Flammability and Safety:** Hydrogen's high flammability is a significant safety concern.  It can form explosive mixtures with air.\n",
      "*   **Industrial Uses:**\n",
      "    *   **Ammonia Production (Haber-Bosch process):** A key ingredient in fertilizers.\n",
      "    *   **Petroleum Refining:** Used in hydrocracking and hydrodesulfurization.\n",
      "    *   **Methanol Production:** Used to create methanol from carbon monoxide.\n",
      "    *   **Hydrogenation:** Used to convert unsaturated fats into saturated fats.\n",
      "    *   **Rocket Fuel:** Liquid hydrogen is used as a rocket fuel due to its high energy-to-weight ratio.\n",
      "*   **Fuel Cells:** Hydrogen is a promising fuel for fuel cells, which convert chemical energy directly into electrical energy with water as the only byproduct. This is a key area of research for clean energy technologies.\n",
      "*   **Emerging Technologies:**  Hydrogen is being explored as a potential energy carrier for a hydrogen economy.  This would involve producing, storing, and using hydrogen as a fuel source.  Challenges include efficient and sustainable production methods, safe storage and transportation, and infrastructure development.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "Hydrogen is a unique element due to its simple structure. Its properties, like low density, high flammability, and ability to form strong covalent bonds, make it useful in a wide range of applications, from industrial processes to potential clean energy technologies. Its simplicity also makes it a fundamental building block of the universe.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash-001\",\n",
    "                             api_key=gemini_key)\n",
    "\n",
    "response = llm.invoke(\"Explain about the chemical structure and properties of Hydrogen\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a0f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12db1df3",
   "metadata": {},
   "source": [
    "### Build simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1fc640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load PDF and split into pages\n",
    "pdf_path = r\"C:\\Users\\Moorthy\\Downloads\\Transformer based 3D tooth.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c7cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pages into smaller text chunks (to fit LLM input limits)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4d4d834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Transformer based 3D tooth \\nsegmentation via point cloud \\nregion partition\\nYou Wu1, Hongping Yan1\\uf02a & Kun Ding2\\nAutomatic and accurate tooth segmentation on 3D dental point clouds plays a pivotal role in computer-\\naided dentistry. Existing Transformer-based methods focus on aggregating local features, but fail to \\ndirectly model global contexts due to memory limitations and high computational cost. In this paper, \\nwe propose a novel Transformer-based 3D tooth segmentation network, called PointRegion, which \\ncan process the entire point cloud at a low cost. Following a novel modeling of semantic segmentation \\nthat interprets the point cloud as a tessellation of learnable regions, we first design a RegionPartition \\nmodule to partition the 3D point cloud into a certain number of regions and project these regions as \\nembeddings in an effective way. Then, an offset-attention based RegionEncoder module is applied \\non all region embeddings to model global context among regions and predict the class logits for \\neach region. Considering the irregularity and disorder of 3D point cloud data, a novel mechanism is \\nproposed to build the point-to-region association to replace traditional convolutional operations. The \\nmechanism, as a medium between points and regions, automatically learns the probabilities that \\neach point belongs to its neighboring regions from the similarity between point and region features, \\nachieving the goal of point-level segmentation. Since the number of regions is far less than the number \\nof points, our proposed PointRegion model can leverage the capability of the global-based Transformer \\non large-scale point clouds with low computational cost and memory consumption. Finally, extensive \\nexperiments demonstrate the effectiveness and superiority of our method on our dental dataset.\\nArtificial Intelligence (AI) technology is improving by leaps and bounds. For stomatology tasks, like digital \\ndentistry, orthodontics and dental implants, learning-based 3D tooth segmentation is a crucial step for a \\ncomputer-aided-design (CAD) system to automatically and accurately identify individual tooth and gingiva \\nbased on 3D real oral scanning data or dental model. This will facilitate the subsequent diagnosis and treatment \\nand free the dentists from tedious manual segmentation work.\\nHowever, due to noise, irregularity, high redundancy, nonuniform distribution and disorder of 3D point cloud \\ndata, 3D point cloud segmentation is full of challenges. Traditional 3D point cloud segmentation methods 1–3 \\nhave poor accuracy and meanwhile cannot be extended to large-scale data. Driven by the tremendous success \\nof 2D computer vision, deep learning-based technology has become the preferred choice for 3D segmentation \\ntasks.\\nRecently, Transformer architecture has been successfully applied in natural language processing 4–6 and 2D \\ncomputer vision tasks7,8 due to its strong advantage in learning long-range dependencies of input sequences, \\nand exhibits better performance than convolutional neural networks (CNNs). As the core of Transformer, \\nself-attention mechanism5 has been proven suitable for processing 3D point clouds, and some Transformer-\\nbased works have emerged for point cloud learning. Most of these works 9–15 use Transformer as an auxiliary \\nmodule with the aim of achieving feature aggregation in the local patch, and only a small proportion 16–18 \\npropose Transformer-based architectures that apply global attention on the whole point cloud while avoiding \\nthe overhead of constructing local neighborhoods. Nevertheless, the global attention mechanism makes these \\napproaches unsuitable for processing large-scale point clouds, e.g., dental point clouds with a scale of 105, due \\nto the heavy computation burden and unacceptable memory consumption.\\nIn this paper, we propose a novel Transformer-based 3D tooth segmentation network, called PointRegion, \\nwhich successfully lowers the computation complexity and memory consumption from the perspective of \\nreducing the input sequence length. Our work is inspired by 2D image segmentation. To reduce inference time and \\nmemory consumption while retaining the accuracy, Zhang et al.19 proposed a decoder-free Vision Transformer \\n(ViT) based segmentation method, which treats an image as a set of learnable regions and obtains pixel-level \\n1School of Information Engineering, China University of Geosciences(Beijing), Beijing 100083, China. 2State Key \\nLaboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of \\nSciences, Beijing 100190, China. \\uf02aemail: yanhp@cugb.edu.cn\\nOPEN\\nScientific Reports |        (2024) 14:28513 1| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='result by a convolution-based correlation mechanism. Compared to per-point prediction on dense regular grids, \\ninterpreting an image as a set of interrelated regions is closer to the essence of semantic segmentation. We \\nbelieve that the same applies to 3D point cloud semantic segmentation. However, since point cloud and image \\nare quite different data, the mechanisms of patch embedding in ViT and correlation computation using standard \\nconvolutions are not directly applicable to 3D point clouds. Therefore, we make some analysis and adjustments \\nlisted as follows:\\n• RegionPartition module Unlike image pixels, point clouds are disordered and unstructured. Therefore, we \\nfirst need to explore a partitioning method suitable for point clouds and then project the point cloud into a \\nsequence of region embeddings. Although the related works 14,17,18 introduced in Section Grid-based Meth -\\nods for Point Cloud Learning have the practice of converting a point cloud into local patches, the points in \\nthe overlapping area of these patches will cause ambiguity when obtaining per-point label. Based on the fact \\nthat the closer points generally have closer semantic information, we propose an effective non-overlapping \\npartitioning method in the RegionPartition module. Moreover, we expand the receptive field of each point \\nduring feature aggregation to compensate for the information loss caused by region partition, which will be \\nintroduced in detail in Section RegionPartition Module.\\n• RegionEncoder module In our PointRegion framework, after the RegionPartition module, the region-level \\nembedding instead of the point-level embedding is input to the attention layer, i.e., the RegionEncoder mod-\\nule, which means the computation complexity is quadratic with respect to the number of regions instead of \\npoints. It is worth noting that the number of regions in our method is far less than the number of points, \\nwhich makes our method suitable for processing large-scale dental point clouds. Taking a point cloud with \\nN=10240 points as an example, existing Transformer-based methods typically treat it as an input sequence for \\nfeature learning, with the input sequence being N. Although mechanisms like ViT exist to reduce the length of \\nthe input sequence, they are not suitable for processing irregular 3D point clouds. Our PointRegion employs \\nthe RegionPartition module to divide the point cloud into 1024 regions, leveraging the powerful local and \\nnon-local feature extraction capabilities of EdgeConv to preserve as much structural information of the point \\ncloud as possible. Subsequently, the RegionEncoder module accepts the 1024 region embeddings as the input \\nsequence, predicting category probabilities for each region. At this point, the input sequence is reduced by \\na factor of ten compared to the original input point cloud. Since the computational complexity and spatial \\noccupancy of the self-attention mechanism grow quadratically with the input sequence, the computational \\ncomplexity and spatial occupancy of PointRegion when using Transformers for point cloud feature extraction \\nare also significantly reduced. The experimental results in Table 2 confirm this point.\\n• Point-to-region association After modeling inter-region relations using the global Transformer, the main \\ndifficulty lies in generating per-point segmentation result with per-region prediction. The affinity head is a \\nmechanism based on standard convolution to describe region geometries by local pixel-region association as \\na corresponding solution in this work19. However, it is not suitable for 3D point clouds. To this end, a novel \\nmechanism is designed to establish point-to-region association by utilizing information similarity between \\npoints and regions. On the basis of region logits generated by the RegionEncoder module, our point-to-re -\\ngion association mechanism can help our model learn the probabilities of each point belonging to the current \\nreference region and its neighbourhoods, which enables us to segment teeth in a per-region instead of per-\\npoint prediction fashion as opposed to conventional UNet-style models9,20–22.Based on the above analyses, we \\npropose a PointRegion network, which successfully extends the 2D segmentation method19 to 3D large-scale \\npoint cloud segmentation task. By extensive experimental evaluation, the experimental results demonstrate \\nthe excellent performance of the proposed deeping learning network on our dental dataset. Furthermore, to \\nthe best of our knowledge, none of the previous works explored the use of relatively complex Transformer \\nstructures in 3D tooth segmentation tasks.\\nIn summary, the main contributions of our work include the followings:\\n• We propose a novel, accurate, and efficient Transformer-based 3D tooth segmentation network, namely, Poin-\\ntRegion, which is capable of learning on large-scale dental data without expensive computational cost and \\nhuge memory consumption.\\n• We design a RegionPartition module to partition the 3D point cloud into a certain number of non-overlap -\\nping regions and then learn region embeddings in an effective local-nonlocal way.\\n• Additionally, we propose a novel mechanism for establishing association between points and regions. This \\npoint-to-region mechanism enables our model to achieve point-level tooth segmentation in a more effective \\nper-region prediction fashion.\\n• Last but not least, extensive experiments and ablative analysis are conducted on our collected and annotat -\\ned dental dataset. Compared to other state-of-the-art methods, our proposed method performs well and \\nachieves superior results.\\nRelated work\\nConsidering whether to pre-process 3D point cloud data, most existing learning-based 3D point cloud segmentation \\nmethods can be roughly classified into grid-based methods 23–31 and point-based methods 9–18,20–22,32–34. Due \\nto the disorder and irregularity of point cloud data, the grid-based methods typically convert point clouds \\ninto regular 2D/3D grid representations through projection or voxelization, and then utilize CNNs for feature \\nlearning. In contrast, the point-based methods learn directly on the original structure of point clouds, effectively \\navoiding the time penalty and inevitable information loss caused by grid transformation.\\nScientific Reports |        (2024) 14:28513 2| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Grid-based methods for point cloud learning\\nAs the foundation of deep learning, CNN has demonstrated powerful learning ability in various 2D visual \\ntasks35–39. In order to explore its potential on 3D point clouds, some researchers attempt to transform irregular \\npoints into intermediate grid structure. The 2D grid-based methods23,26–29,31,40–42 typically project a point cloud \\ninto 2D images. Among them, Lawin et al. 26 proposed to first leverage virtual cameras to obtain multi-view \\nimages of a point cloud, and then performed pixel-wise prediction on the images of each view using a multi-\\nstream CNN. These predicted scores are further fused to obtain the final semantic labels for each point. Instead of \\nchoosing the positions of multiple cameras, Squeezeseg29 and its improved version SqueezesegV231 put forward \\nprojecting the point cloud onto a sphere and processing its spherical representation with an end-to-end pipeline \\nbased on CNNs. Although the good scalability of these methods enables them to handle large-scale point clouds, \\ntheir performance is sensitive to the selection of projection plane, and meanwhile occlusion issues inevitably \\nresult in information loss. It is difficult to select an appropriate projection plane in practice.\\nIn the case of 3D grid-based methods24,25,30,43, point clouds are usually voxelized to dense grid representations. \\nHowever, using standard 3D convolutions directly on voxels makes the memory consumption and computation \\ngrow cubically with the voxel resolution, thereby limiting the performance of this type of methods. In response \\nto this challenge, approaches based on sparse convolution 44,45 and tree structures 46–49 have emerged. These \\nsolutions have indeed reduced learning costs and enhanced performance, but the loss of geometric details \\nduring voxelization cannot be ignored.\\nPoint-based methods for point cloud learning\\nCompared with the grid-based methods, the point-based methods construct a deep network architecture learned \\ndirectly on the point cloud, which can preserve geometric structure of the point cloud to the greatest extent. As a \\nmilestone in deep learning-based point cloud processing, PointNet32 uses shared multi-layer perceptrons (MLPs) \\nto aggregate features independently on each point followed by a permutation-invariant max-pooling operation. \\nHowever, PointNet is unable to capture local features and cannot perform well in complex tasks like segmentation, \\ntherefore, a hierarchical PointNet ++20 has been designed. Inspired by PointNet 32 and PointNet ++20, many \\nsubsequent works50–54 borrow from hierarchical structures and aggregate local neighborhood information in \\na pooling manner.\\nA different approach redefines effective convolution operators for point clouds. Li et al.21 proposed X-Conv \\nconsisting of X-transformation and typical convolution operators in PointCNN. It successfully extends CNN to \\nfeature learning of irregular point clouds. PointConv22 treats convolution kernels as nonlinear functions of the \\nlocal coordinates of 3D points comprised of weight and density functions. Thomas et al. 33 presented KPConv, \\nin which a set of kernel points are used to carry weights, and correlation coefficients are determined by the \\npositional relationship between the input points and the kernel points. In addition, the deformable version of \\nKPConv is more flexible, robust, and suitable for complex tasks.\\nAccording to the operating scale, Transformer-based methods can be classified into local-based Transformers \\nand global-based Transformers. Local-based Transformers 9–15,55–58 make full use of the attention mechanism \\nthat is a natural fit for point clouds, and design an attention-based module for local geometric feature learning. \\nZhao et al.9 proposed Point Transformer (PT) to capture the local information by the self-attention vector, which \\nis proven to be more effective compared to the scalar self-attention. Inspired by Swin Transformer8, Zhang et al.14 \\nand Lai et al.15 proposed to convert point cloud into independent 3D grids through voxelization and then build \\nthe local self-attention within each window. On the contrary, global-based Transformers 16–18,34,59,60 are used \\nto learn the global context features. Inspired by the nonlocal operation, Y an et al. 34 presented a local-nonlocal \\nmodule to capture the neighbor and long-range dependencies of sampled points. Point Cloud Transformer \\n(PCT) proposed by Guo et al.16 stacks four global attention layers to learn semantic information, and the offset-\\nattention in PCT instead of the original self-attention is able to sharpen the attention weights and reduce the \\ninfluence of noise. Following ViT7, Point-BERT17 and Point-MAE18 first grouped point cloud into several local \\npatches and adopted a mini-PointNet32 to project them into point embeddings, which are then fed to pre-train \\nthe global-based Transformers. Overall, local-based Transformers are applicable to large-scale point cloud \\nprocessing, but multiple construction of local patches for irregular data has become a bottleneck in efficiency. \\nIn contrast, the huge memory and computational costs make it difficult for global-based Transformers to deal \\nwith large-scale data.\\nDeep learning segmentation methods for dental point clouds\\nIn addition to general-purpose point cloud segmentation methods, various deep learning models have been \\nproposed for tooth segmentation on 3D intraoral scanned (IOS) data recent years. To preserve the topological \\nstructure and geometric details of 3D ISO data as much as possible, most of these methods are implemented \\nbased on points, reflecting the current research trend in the field.\\nA majority of these methods are fully supervised61–67 with the exception of a few which are weakly supervised \\nor semi-supervised68–70. Zanjani et al. 63 introduced a Mask-MCNet framework which localizes each tooth by \\npredicting its 3D bounding box and simultaneously segments the points that belong to each individual tooth \\ninstance. Lian et al.61 evaluated MeshSegNet which learns global information from the tooth point cloud using \\na PointNet similar network and the local information is learnt using different scale of adjacency matrices. This \\nnetwork borrows heavily from PointNet architecture but stands out by its use of adjacency matrix for understanding \\nthe local geometrics. Zhang et. al 62 proposed a method which learns separately from the coordinates and the \\nnormals information by employing a parallel network. These methods discussed herein have been trained and \\nevaluated on relatively small dataset of annotated 3D intraoral scans (starting 30 from to 120), with a dearth \\nof sample diversity, predominantly focusing on fully aligned normal dentition. This limitation hampers their \\napplicability in real-world clinical scenarios, where a broader range of dental configurations and anomalies are \\nScientific Reports |        (2024) 14:28513 3| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='encountered. In addition, recent weakly supervised and self-supervised methods have garnered widespread \\nattention due to their ability to learn from limited data. DArch68 proposes a weakly annotated training approach \\nto train the segmentation network using only a few annotated teeth from each dental model. However, the \\nfirst stage of DArch still requires complete centroid annotations, which is challenging to obtain in the absence \\nof dense point-wise annotations. Despite the reduced dependence on manual annotation provided by weakly \\nsupervised or semi-supervised methods, supervised approaches generally demonstrate stronger generalization \\ncapabilities. Furthermore, when evaluating the outcomes, the segmentation results from supervised are notably \\nmore precise, both in terms of accuracy and visual performance, which is essential for clinical dental diagnostics \\nand treatment processes. Due to the fact that most fully supervised methods are based on MLP and convolution, \\nin order to fully leverage the advantages of Transformer, this paper attempts a point cloud segmentation method \\nbased on Transformer.\\nMaterials and methods\\nTheoretical foundation\\nDeep Learning, a subfield of machine learning, has made significant advancements in recent years across \\nvarious domains such as image recognition, speech recognition, and natural language processing. The core of \\nthis technology lies in constructing multi-layer neural network models that can automatically learn complex \\nfeatures from data through extensive training. With the evolution of deep learning techniques, their potential in \\nprocessing 3D data has also become increasingly evident, particularly in the field of point cloud segmentation.\\nPoint cloud and mesh are two common methods for representing 3D data. Point cloud, as a collection \\nof points in 3D space, describes the geometric shape of an object by recording the coordinates of each point \\n(typically x, y, z coordinates). Mesh, on the other hand, describe the surface of an object by constructing a \\nnetwork of polygons through vertices, edges, and faces.\\nIn the application of point cloud segmentation, deep learning models typically require a large amount \\nof annotated data for training. This data includes not only the point cloud itself but also the corresponding \\nsegmentation labels, indicating which part of the object each point belongs to. Through supervised learning, \\ndeep learning models can learn the mapping relationship from point cloud data to segmentation labels. As \\ntraining progresses, the model’s segmentation capabilities are gradually strengthened, ultimately enabling \\naccurate segmentation of new point cloud data.\\nOverview\\nIn this paper, we design a PointRegion model for 3D tooth segmentation. By interpreting the point cloud as a \\ntessellation of learnable regions, our model not only exploits Transformer to learn global context information, \\nbut also segments tooth in an effective way. The overview of our PointRegion model is shown in Fig. 1.\\nSince our original dental model is mesh data, we need to convert it into point cloud. In Section Mesh2Point, \\nwe first detail the process of converting mesh into points. Then,\\xa0Section RegionPartition module and Section \\nFig. 1. Overall architecture of PointRegion. The dental mesh is first transformed into point cloud through \\nMesh2Point, then the RegionPartition module divides the point cloud into several regions and projects these \\nregions into region embeddings. The region embeddings are input to the RegionEncoder module to predict \\nregion logits. To establish the association between points and regions, the probabilities that each point belongs \\nto neighboring regions are learned from the intermediate output features (red dashed arrow) of the two \\nmodules. Based on the probability matrix, the point-wise class logits are output in a weighted summation \\nmanner. Finally, the graph-cut algorithm is further applied to improve segmentation performance.\\n \\nScientific Reports |        (2024) 14:28513 4| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='RegionEncoder module respectively detail our RegionPartition module and RegionEncoder module for region \\nfeature learning. In Section Point level segmentation based on point and region association, we describe \\nthe method of mapping region-wise prediction to point-wise prediction and the solution to further refine \\nsegmentation results.\\nMesh2Point\\nGiven an input mesh dental model, we sample it to obtain the point cloud consisting of N points, each of \\nwhich has d-Dimensional attributes. In the simplest case of d =3 , each point is described by 3-Dimensional \\ncoordinates of the corresponding mesh cell center. Moreover, it is also possible to include additional information \\nsuch as the 3-Dimensional normal vector of the cell surface and the 9-Dimensional coordinates of the cell’s three \\nvertices. The values of different attributes vary greatly, therefore, in order to reduce data differences and achieve \\nmore stable network optimization, we perform min-max normalization along each dimension of the point cloud \\ndata. We denote the normalized point cloud as P = {pi : pi ∈ Rd,i =1 ,...,N }, which serves as the input to \\nthe network.\\nRegionPartition module\\nInspired by Dynamic Graph CNN (DGCNN) 50, we design a RegionPartition module to divide a point cloud \\ninto multiple non-overlapping regions and learn region embeddings from the local-nonlocal point features. As \\nshown in Fig. 2, this module is composed of two branches, the upper branch is used for dividing the point cloud \\ninto regions and the lower branch is responsible for extracting regional feature embedding.\\nDivision of regions\\n Firstly, according to the 3D coordinates of points, we select G points from the original point cloud P∈ RN×d \\nrespectively as the initial center points of G disjoint regions by using the farthest point sampling (FPS) algorithm. \\nWe denote the set of these G initial center points as Cr = {pi : pi ∈ R3,i =1 ,...,G }⊂P , where pi is the \\n3D coordinates of the i-th point. Then, G disjoint regions are formed by different number of points nearest to \\neach selected center point. For now, point cloud P can be represented as R = {Ri : i =1 ,...,G }, where \\nRi ∩ Rj = ∅, ∀i, j∈ [1,G ],i ̸=j. This region division method is referred to as nearest neighbor clustering \\n(NNC) in our work. This method can help our model avoid conflicts during the point-to-region mapping phase \\nmentioned in Section Point level segmentation based on point and region association.\\nExtraction of regional feature embedding\\n Since the point cloud is divided into several regions, it is desirable to significantly increase the receptive field \\nfor each point before aggregating point features into embeddings, such that more geometric details of the input \\npoint cloud can be more likely preserved. To this end, we adopt cascaded EdgeConv block proposed in DGCNN \\nto learn point features F p = {fp\\ni : fp\\ni ∈ Rde ,i =1 ,...,N }, where fp\\ni  is a de-Dimensional feature vector for \\neach point pi ∈P . Thanks to dynamic graph update of DGCNN, each point can have a greater and nonlocal \\nreceptive field by concatenating multiple EdgeConv blocks. More specifically, suppose that the range of receptive \\nfield of each point after the first EdgeConv block is k, then each point can extract information from k2 points after \\nthe second concatenated block. Then, based on the partition of the points from the division of regions branch, we \\naverage all the point features in one region as the final region embeddings F e = {fe\\ni : fe\\ni ∈ Rde ,i =1 ,...,G }\\n, where fe\\ni  is the region embedding of the region Ri. This can be formulated as Eq. (1) :\\nFig. 2. Details of RegionPartition module. Black dots represent the sampled points, green solid circles \\nrepresent the regions which the point cloud is divided into.\\n \\nScientific Reports |        (2024) 14:28513 5| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='fe\\ni =\\n∑\\npj ∈Ri\\nfp\\nj\\nmi\\n,i =1 ,...,G , j =1 ,...,N,  (1)\\nwhere mi is the number of points in the region Ri. If the feature fp\\nj  in Eq. (1) is 3D coordinates of point pj , we \\ncan update the set of regional center points Cr = {pr\\ni : pr\\ni ∈ R3,i =1 ,...,G }.\\nRegionEncoder module\\nIn order to predict the class logits for each region, we propose a RegionEncoder module that takes region \\nembeddings as its input. The module is a Transformer-based structure, and we use offset-attention mechanism16 \\ninstead of self-attention5 to learn the global context of the point cloud by directly modeling inter-region relations.\\nLet Fin and Fout be the input and output of offset-attention (OA) block, the OA block can be represented \\nas follows:\\n (Q, K, V)= Fin · (Wq,Wk,Wv),  (2)\\n Fsa = l1-norm\\n(\\nsoftmax\\n(\\nQ · KT))\\n· V,  (3)\\n Fout = LBR(Fin − Fsa)+ Fin,  (4)\\nwhere Wq, Wk, Wv are the shared learnable linear transformation, and Q, K, V are respectively the query, key \\nand value matrices. LBR is a feed-forward neural network composed of Linear, BatchNorm and ReLU layers in \\nsequence. For the normalization of weights in Eq. ( 3), different from the standard self-attention that adopts a \\nscaling strategy in the batch dimension and softmax in the column dimension, OA uses softmax operations on \\nthe row dimension and l1-norm on the column dimension. Moreover, the attention features in OA is replaced \\nby the offset between the attention features and the input features in Eq. ( 4) inspired by graph convolution \\nnetworks71.\\nFigure 3 shows the implementation details of RegionEncoder module. The region embeddings Fe are first \\nprojected to a high-dimensional latent space using a shared MLP for the input of a cascaded L layers of OA \\nblocks with residual connection. Then the outputs of all OA blocks are concatenated to be the input of a shared \\nMLP . Hereafter, we can get the region-wise feature representation Fmr formulated as Eq. (5):\\n F mr = MLP\\n(\\nConcat\\n(\\n{F r\\nl }L\\nl=1\\n))\\n,  (5)\\n F r\\n1 = OA1 (MLP(F e)) ,  (6)\\n F r\\nl = OAl (F r\\nl−1) ,l =2 ,...,L ,  (7)\\nwhere OA  represents the l-th OA block as described in Eqs. ( 2)-(4), and its output F r\\nl = {fr\\nl,i,i =1 ,...,G } \\nserves as the input for the next OA block.\\nIn order to extract global feature vectors g, we introduce global prior information of maxillary and mandible \\nencoded as a one-hot categorical vector according to our dental dataset. This vector is first processed by MLP \\nand then concatenated with the max-pooling and mean-pooling vectors of Fmr. In this way, we can effectively \\navoid assigning maxillary (or mandible) labels to mandible (or maxillary). Following most other segmentation \\nFig. 3. Details of RegionEncoder Module. MaxP and MeanP represent max pooling and mean pooling \\noperations, respectively.\\n \\nScientific Reports |        (2024) 14:28513 6| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='works16,20,32,50, the global features g are repeated G times first, and then concatenated with region-wise \\nrepresentation Fmr. The concatenated feature vectors are processed by a region classifier to predict region logits \\nY r = {yr\\ni : yr\\ni ∈ RC,i =1 ,...,G }, where C is the number of semantic labels.\\nPoint level segmentation based on point and region association\\nTo achieve the goal of point level segmentation, we propose a learnable mechanism to establish the association \\nbetween each point and all G non-overlapping regions obtained in Section RegionPartition module. This \\nmethod is suitable for unstructured point clouds and also crucial for achieving fine-grained point cloud \\nsegmentation based on coarse-grained region prediction. For each point pi ∈P , we can build its association \\nwith Rj ∈R ,j =1 ,...,G  and calculate its class logits through the following three steps. The implementation \\ndetails of the first two steps are shown in Fig. 4.\\nSearching for K-nearest neighbor regions (SKNR)\\nFor each point pi, we define its neighboring regions according to Euclidean distances between the point and \\nthe centers Cr of all G regions obtained from Eq. (1). Via K-nearest neighbor (KNN) algorithm, we can find K \\nneighboring regions of point pi in R. We use Npi = {Nk : Nk ∈{ 1,...,G },k =1 ,...,K } formulated as \\nEq. (8) to record the indices of the K neighboring regions of point pi:\\n Npi = KNN(pi, Cr). (8)\\nLearning point-to-region probability (LP)\\n After that, to quantify point-region association, the probabilities {sk(pi,R Nk ): k =1 ,...,K } that pi belongs \\nto its neighbouring regions {RNk : RNk ∈R ,k =1 ,...,K } need to be learned. 19 took the convolution \\nmodule followed by a softmax to produce normalized probabilities, but it is not feasible for irregular point \\nclouds. In our work, we design a shared function M(·) to learn the probability for each point-region pair based \\non the similarity between the point feature and its neighboring regional features, and the similarity operation is \\ndenoted as sim(·). This step is formally defined as follows:\\n sk(pi,R Nk )= M(sim(fpm\\ni ,f r\\nl,Nk ),W ), (9)\\nwhere the function M(·) is a shared MLP followed by softmax,  W is the learnable weights of the shared MLP \\nand fpm\\ni  is the point feature of the point pi obtained from fp\\ni  (see Section RegionPartition module) through \\nshared MLPs for efficiency. The feature of the region with the k-th neighbouring region index Nk ∈N pi  is \\nselected from one of the outputs of L OA blocks in the RegionEncoder module, for one, fr\\nl,Nk , i.e., the k-th \\nneighboring region feature output by the l-th OA block. All sk(pi,R Nk ) form a probability matrix S ∈ RN×K.\\nFig. 4. Details of the mechanism of point-to-region association. (a) Illustration of searching for K-nearest \\nneighbor regions, where the red dot stands for a 3D point pi within the region Rg. (b) Illustration of learning \\npoint-to-region probability.\\n \\nScientific Reports |        (2024) 14:28513 7| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Calculating point-wise class logits through weighted summation\\nFinally, with the help of the region logits Y r introduced in Section RegionEncoder module, we calculate the \\nclass logits for each point pi, denoted as Y p = {yp\\ni : yp\\ni ∈ RC,i =1 ,...,N }, through weighted summation \\nas Eq. (10):\\n \\nyp\\ni =\\nK∑\\nk=1\\nsk(pi,R Nk ) · yr\\nNk . (10)\\nThe point pi is labeled as the class with the maximum class logits value. To further improve segmentation \\nperformance and refine the coarse segmentation boundaries, we use the graph-cut algorithm 72 to post-process \\nthe segmentation results.\\nExperimental evaluation\\nDataset and metrics\\nTo evaluate our model, we collected a set of tooth mesh models from the real-world clinics, which contain a \\nvariety of dental cases, such as missing teeth, tooth deformities and post orthodontic teeth. The dental model \\ndataset is first manually labeled by ten people using Mesh Labeler software (version 3.4)73, and then two people \\nvalidate the annotated data. The entire dataset consists of 916 dental models (403 maxillaries and 513 mandibles), \\nwith each mesh model containing an average of 100,000 faces. We randomly and evenly split it into 815 models \\nfor training and 101 for evaluation.\\nTo evaluate the performance of our method, we use the mean intersection over union (mIoU), the overall \\naccuracy (OAcc) as effectiveness evaluation criteria. Besides, we also use the Giga floating point operations \\n(GFLOPs) to measure the efficiency of different methods.\\nImplementation details\\nOur segmentation networks are trained with PyTorch on a NVIDIA TITAN RTX 24GB GPU for 200 epochs, \\nwe use Adam optimizer with an initial learning rate of 10−3 and a weight decay of 10−4 to minimize the cross-\\nentropy segmentation loss. For better training, we reduce the learning rate to half of the initial learning rate every \\n20 epochs.\\nDue to limitations of GPU memory, it is hard to input all points ( 105 on average) into the network for \\ntraining. We split all points into three sub-samples including 10240 points using FPS and set batch size as 2. \\nDuring testing, due to the inconsistent number of points within each sample, in order to obtain their predictions, \\nwe also use multiple FPS to get multiple sub-samples with the size of 10240 until the number of the remaining \\npoints is less than 20480, and the batch size for testing is set as 1.\\nExperimental results\\nComparison with state-of-the-art architectures\\nWe make comparisons with state-of-the-art semantic segmentation methods on our dental dataset. The \\nexperimental results are shown in Table 1. Apart from the pioneering works PointNet 32 and PointNet ++20, \\nwe also choose representative innovative works from each period, including DGCNN 50 and PCT 16 involved \\nin our model. PointNet and PointNet ++, as early point cloud processing networks, exhibit shorter inference \\ntime and smaller parameter size, which may limit their ability to learn complex features and thus affect overall \\nperformance. Note that our PointRegion outperforms the other methods in terms of both OAcc and mIoU. \\nEspecially after boundary optimization using graph-cut algorithm 72, we achieve 97.109% OAcc and 92.511% \\nmIoU, respectively, surpassing PCT by 0.789% and 3.547%. In addition, compared to benchmark 50 and \\nTransformer-based methods14,16, PointRegion achieves significant improvement in inference time at the cost \\nof a small increase in parameter size, demonstrating good efficiency. All the improvements are attributed to its \\n Method\\nParameter Inference OAcc(%) mIoU(%)\\nSize(M) Time(ms) Maxillary Mandible All Maxillary Mandible All\\nPointNet32 3.531 50.960 60.091 68.005 64.423 27.512 49.737 40.055\\nPointNet++20 1.756 54.968 91.659 91.807 91.740 76.803 72.930 74.617\\nPointConv22 12.779 119.254 93.389 94.496 93.995 80.717 81.149 80.961\\nDGCNN50 1.462 141.980 94.905 96.108 95.564 85.346 89.939 87.938\\nPCT16 2.482 123.768 96.538 96.139 96.320 88.801 89.090 88.964\\nPVT14 2.690 127.880 94.718 94.523 94.612 84.292 85.883 85.190\\nPointMLP52 15.296 114.900 95.999 95.877 95.932 87.561 88.864 88.297\\nPointRegion w/o post-process 2.490 90.288 96.866 96.729 96.791 90.739 91.614 91.233\\nPointRegion with post-\\nprocess – – 97.336 96.921 97.109 92.775 92.308 92.511\\nTable 1. Results of the state-of-the-art methods. None of the comparison methods (the first seven rows) \\nemploy post-processing technique to optimize the segmentation results. Significant values are in bold.\\n \\nScientific Reports |        (2024) 14:28513 8| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='partitioning strategy, which not only reduces input sequences, effectively reducing computational complexity, \\nbut also makes it easier for Transformer to learn differences between regions and get richer representations. \\nTherefore, our RegionPartition module results in better performance.\\nAblative analysis\\nModel hyperparameters  During training process, there are two model hyperparameters, i.e., number of re -\\ngions, G, in the RegionPartition module and number of nearest neighbor regions, K, when establishing point-\\nto-region association. In order to evaluate the impact of the two model hyperparameters on our model perfor -\\nmance, we first fix K to 32 and vary G from 128 to 4096, and then fix G to 1024 while changing K. The results \\nwith different G or K are reported respectively in Table 2 and Table 3.\\nResults in Table 2 show that the performance of our method can be improved as G increases. When G is small, \\nsuch as G = 128, the performance of segmentation is worse, with a gap of 2.39% mIoU from the best result when \\nG is set to 1024. The fewer number of regions means the more diverse categories of points in each region, which \\nmakes it difficult to learn discriminative features. Furthermore, too large value of G can also lead to a decrease \\nof OAcc and mIoU. The potential reason may be that excessive input sequences become a burden for modeling \\nthe correlation between regions. In the extreme case, where the number of regions is equal to the number of \\ninput point clouds (10240), we eliminate the branch of division of regions within the RegionPartition module \\nand Point-to-Region Association mechanism, resulting in a significant increase in memory and computation \\ncompared to G=1024. Therefore, we set G to 1024 in the rest experiments.\\nUnlike regular pixels of 2D image, points of 3D point cloud are disordered, which means that one point \\nmay have a different number of neighbouring regions. In our method, we use KNN algorithm to find K regions \\nnearest to each point. As shown in Table 3, our method performs best when K = 32. Note that the performance \\nof our method drops significantly if K = G. Therefore, we can get the same conclusion as in this work 19 that \\nthere is no necessary to consider all regions as neighbouring regions.\\nPost-process hyperparameter  As well known, the tuning parameter λ in the graph-cut algorithm 72 is used \\nto balance the contribution of the data fitting term and the local smoothing term in the optimization objective \\nfunction. We test the impact of different values of λ on the final results in Table 4. In order to achieve the best \\neffect of boundary refinement, we adjust the value of λ with a step size of 5 within the range of 0 to 60. Note that \\nλ =5  can help mIoU and OAcc higher and make the boundary smoother.\\nSelection of region features  In Section Point level segmentation based on point and region association, we se-\\nlect the output of one of the cascaded L OA blocks as region features for building the point-to-region association. \\nK GFLOPs OAcc (%) mIoU (%)\\n8 37.216 96.641 90.098\\n16 37.244 96.666 90.156\\n24 37.272 96.741 90.766\\n32 37.299 96.791 91.233\\n40 37.327 96.634 90.904\\n48 37.355 96.503 90.709\\n56 37.382 96.654 90.487\\n64 37.410 96.521 90.268\\n··· ··· ··· ···\\n1024 40.651 96.112 88.439\\nTable 3. Results with varying the number of nearest neighbor regions K. The number of regions is fixed to \\n1024. Significant values are in bold.\\n \\nG Memory (MB) GFLOPs OAcc (%) mIoU (%)\\n128 1473.935 30.204 95.861 88.841\\n256 1487.166 31.088 96.622 90.064\\n512 1517.355 32.986 96.698 90.776\\n1024 1596.875 37.299 96.791 91.233\\n2048 1826.914 47.996 96.744 90.728\\n4096 2575.078 77.668 95.705 89.091\\n··· ··· ··· ··· ···\\n10240 7123.664 216.789 96.231 88.944\\nTable 2. Results with varying the number of regions G. The number of nearest neighbor regions is fixed to 32. \\nSignificant values are in bold.\\n \\nScientific Reports |        (2024) 14:28513 9| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='In order to show the impact of the output features at different OA layers on the performance of our method, we \\nconduct a serial of experiments. Considering model size and referring to the setting in PCT 16, we set L to 4. As \\nshown in Table 5, for 4 cascaded OA blocks, selecting the output of the second OA block can help our model \\nachieve optimal performance. This can be attributed to the trade-off between increasing the semantic richness of \\nregion features and reducing the gap between point and region features.\\nPartitioning methods  Partitioning methods can affect the efficiency and performance of our model. Inspired \\nby this method19, we first naturally thought of using voxelization to partition the point cloud by analogy with \\nimages. However, the sparsity of point clouds makes non-empty voxels sparsely distributed and occupy a con -\\nsiderable portion in the voxel domain, thereby limiting the performance of this method. In comparison, NNC \\nhas the following advantages: 1) It can avoid empty regions, where at least one point exists in each region, saving \\ntime consumption like processing empty voxels. 2) This method is more flexible. It is not necessary to set the \\nfixed number of points in each area, and clustering is completed completely according to the spatial semantic \\ninformation of points. So in this experiment, we choose Voxelization and NNC for comparison. For sufficient \\nfairness, we set the resolution of the voxelization method to 8, the number of regions for the NNC method to \\n83 = 512, and set the same number of nearest neighbors for both methods to 33 = 27. The experimental results \\nin Table 6 demonstrate that NCC can improve performance without significantly reducing the computational \\nefficiency of our model.\\nQualitative results\\nPoint-to-region association  As mentioned in Section Materials and methods, our PointRegion model is able \\nto achieve accurately fine-grained point-level segmentation by establishing the association between each point \\nand corresponding regions. In Fig. 5, we take one region from a source point cloud and visualize the probabilities \\nof all the points associated with this region. Whether the region is located on the tooth or on the gingiva, we \\nfound that its association with the relevant points varied inversely with their distances. More specifically, with \\nthe distance between the point and the region increasing, the probability of the point belonging to the region \\nbecomes lower. This is in line with our expectations and confirms that it is reasonable to achieve point-level seg-\\nMethod GFLOPs OAcc (%) mIoU (%)\\nVoxelization 32.251 93.256 83.048\\nNNC 32.969 96.409 90.290\\nTable 6. Results with different partitioning methods. Significant values are in bold.\\n \\nl OAcc (%) mIoU (%)\\n1 96.214 88.663\\n2 96.791 91.233\\n3 96.120 90.021\\n4 96.785 90.691\\nTable 5. Results of selecting region features output by different attention layers. l denotes the index of the \\nattention layer. Significant values are in bold.\\n \\nMethod λ OAcc (%) mIoU (%)\\nModel with post-process\\n0 96.681 91.067\\n5 98.410 92.511\\n10 98.150 92.480\\n15 98.001 92.403\\n20 97.882 92.287\\n25 97.799 92.207\\n30 97.722 92.101\\n35 97.654 92.023\\n40 97.598 91.946\\n45 97.538 91.860\\n50 97.496 91.792\\n55 97.445 91.706\\n60 97.399 91.640\\nTable 4. Results with varying tuning parameter λ. Significant values are in bold.\\n \\nScientific Reports |        (2024) 14:28513 10| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='mentation with low memory consumption and computational cost based on region logits and point-to-region \\nprobabilities.\\nSegmentation results in different dental cases  We visualize the segmentation results of our method and PCT16 \\nin various dental cases, as shown in Fig. 6. Note that our PointRegion can accurately recognize gingiva and indi-\\nvidual teeth, and performs well in situations such as the standard teeth, the crowded teeth, the small unerupted \\nteeth, the missing teeth and the asymmetric arch, which demonstrates the effectiveness of our method. From the \\nsecond and third columns, we can see that our method produces visually more continuous results by a per-re -\\ngion prediction fashion than PCT with dense per-point prediction. For ease of description, we divide maxillary \\n(or mandible) into two quadrants by a midline passing through the teeth. Within the left quadrant, the teeth are \\nnumbered from right to left as L1 to L8. Similarly, within the right quadrant, the teeth are numbered from left to \\nright as R1 to R8. Specifically, in Case 2, the crowded arrangement of teeth results in numerous discrete errors in \\nboundary prediction by PCT in the L3-R3 region, with these errors predominantly concentrated around the gin-\\ngiva adjacent to the tooth root. In the case of missing teeth, Case 4, PCT does not recognize the absence of tooth \\nR2, erroneously assigning the R2 label to the gingiva at that location. In contrast, our PointRegion performs well \\nin both scenarios, and exhibits nearly no erroneous segmentation in the R4-R8 and L4-L8 regions, particularly \\nevident in Case 5 with the asymmetric dental arch. Besides, comparing the segmentation results of the last two \\ncolumns, the tooth-tooth and teeth-gingiva boundaries are significantly improved and the discrete erroneous \\npredictions are also corrected by the graph-cut algorithm72.\\nConclusions\\nIn this paper, we present PointRegion, a novel and efficient Transformer based\\xa03D tooth segmentation model. \\nIn order to extend advanced method from 2D image segmentation to 3D point cloud segmentation, we design \\na RegionPartition module for region embedding and a RegionEncoder module for region prediction, as well \\nas an innovative mechanism for establishing point-to-region association. By interpreting a point cloud as a set \\nof learnable regions, we can apply global-based Transformer to large-scale point cloud dataset at a lower cost. \\nDespite our method exhibits outstanding performance both qualitatively and quantitatively on our dental dataset, \\nsegmentation of boundary details remains a challenge. Unsmooth segmentation boundaries and segmentation \\nerrors tends to be more common in cases involving extremely crowded teeth, dental calculus and swollen \\nginigiva. The main reason could be that these case are quite rare, leading to insufficient learning opportunities \\nfor the model. The intricate dental arrangements, overlapping structures and noisy data present significant \\ndifficulties for accurate segmentation. Although the graph-cut post-processing algorithm can effectively \\nimprove the segmentation details at the edges of teeth, the introduction of additional complexity means that \\nmore computational resources and time costs are required, which to some extent affects the practicality of the \\nmethod. Addressing these limitations would help improve the accuracy and efficiency of the dental treatment \\nprocess. Next, we hope to further explore in terms of the boundary segmentation, and design dental software \\nsuitable for professional medical services based on our current work.\\nFig. 5. Visualization of probabilities between specific region and associated points. For each set, Left: The \\nyellow dots indicate the points located within a specific region, while the blue dots represent a set of region \\nassociated points outside the region; Right: Heatmap of the probabilities between a specific region and all its \\nrelevant points.\\n \\nScientific Reports |        (2024) 14:28513 11| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Data availability\\nDue to the involvement of patient privacy, the dental dataset analyzed during the current study is not publicly \\navailable. Upon a reasonable request, the corresponding author will provide access to the data supporting the \\nconclusions of this study.\\nReceived: 10 July 2024; Accepted: 11 November 2024\\nFig. 6. Visualization of segmentation results. Top to bottom: Case 1, 2, 3, 4 and 5. Left to right: Ground Truth, \\nPCT, PointRegion, PointRegion with post-process.\\n \\nScientific Reports |        (2024) 14:28513 12| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='References\\n 1. Fischler, M. A. & Bolles, R. C. Random sample consensus: A paradigm for model fitting with applications to image analysis and \\nautomated cartography. Commun. ACM 24, 381–395 (1981).\\n 2. Jiang, X.\\xa0Y ., Meier, U. & Bunke, H. Fast range image segmentation using high-level segmentation primitives. In Proceedings Third \\nIEEE Workshop on Applications of Computer Vision. WACV’96, 83–88 (1996).\\n 3. Chen, J. & Chen, B. Architectural modeling from sparsely scanned range data. Int. J. Comput. Vis. 78, 223–236 (2008).\\n 4. Bahdanau, D., Cho, K. & Bengio, Y . Neural machine translation by jointly learning to align and translate. arXiv preprint \\narXiv:1409.0473 (2014).\\n 5. Vaswani, A. et al. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing \\nSystems, 6000–6010 (2017).\\n 6. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. \\narXiv preprint arXiv:1810.04805 (2018).\\n 7. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 \\n(2020).\\n 8. Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In 2021 IEEE/CVF International Conference \\non Computer Vision (ICCV), 9992–10002 (2021).\\n 9. Zhao, H., Jiang, L., Jia, J., Torr, P .\\xa0H. & Koltun, V . Point transformer. In 2021 IEEE/CVF International Conference on Computer \\nVision (ICCV), 16239–16248 (2021).\\n 10. Pan, X., Xia, Z., Song, S., Li, L.\\xa0E. & Huang, G. 3d object detection with pointformer. In 2021 IEEE/CVF Conference on Computer \\nVision and Pattern Recognition (CVPR), 7459–7468 (2021).\\n 11. Wu, L., Liu, X. & Liu, Q. Centroid transformers: Learning to abstract with attention. arXiv preprint arXiv:2102.08606 (2021).\\n 12. Gao, Y . et al. Lft-net: Local feature transformer network for point clouds analysis. IEEE Trans. Intell. Transp. Syst. 24, 2158–2168 \\n(2022).\\n 13. Wang, Z., Wang, Y ., An, L., Liu, J. & Liu, H. Local transformer network on 3d point cloud semantic segmentation. Information 13, \\n198 (2022).\\n 14. Zhang, C., Wan, H., Shen, X. & Wu, Z. Pvt: Point-voxel transformer for point cloud learning. Int. J. Intell. Syst. 37, 11985–12008 \\n(2022).\\n 15. Lai, X. et al. Stratified transformer for 3d point cloud segmentation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern \\nRecognition (CVPR), 8490–8499 (2022).\\n 16. Guo, M.-H. et al. Pct: Point cloud transformer. Comput. Visual Media 7, 187–199 (2021).\\n 17. Yu, X. et al. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In 2022 IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition (CVPR), 19291–19300 (2022).\\n 18. Pang, Y . et al. Masked autoencoders for point cloud self-supervised learning. arXiv preprint arXiv:2203.06604 (2022).\\n 19. Zhang, Y ., Pang, B. & Lu, C. Semantic segmentation by early region proxy. In 2022 IEEE/CVF Conference on Computer Vision and \\nPattern Recognition (CVPR), 1248–1258 (2022).\\n 20. Qi, C.\\xa0R., Yi, L., Su, H. & Guibas, L.\\xa0J. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Proceedings \\nof the 31st International Conference on Neural Information Processing Systems, 5105–5114 (2017).\\n 21. Li, Y . et al. Pointcnn: Convolution on X-transformed points. arXiv preprint arXiv:1801.07791 (2018).\\n 22. Wu, W ., Qi, Z. & Fuxin, L. Pointconv: Deep convolutional networks on 3d point clouds. In 2019 IEEE/CVF Conference on Computer \\nVision and Pattern Recognition (CVPR), 9613–9622 (2019).\\n 23. Su, H., Maji, S., Kalogerakis, E. & Learned-Miller, E. Multi-view convolutional neural networks for 3d shape recognition. In 2015 \\nIEEE International Conference on Computer Vision(ICCV), 945–953 (2015).\\n 24. Maturana, D. & Scherer, S. Voxnet: A 3d convolutional neural network for real-time object recognition. In 2015 IEEE/RSJ \\nInternational Conference on Intelligent Robots and Systems (IROS), 922–928 (2015).\\n 25. Tchapmi, L., Choy, C., Armeni, I., Gwak, J. & Savarese, S. Segcloud: Semantic segmentation of 3d point clouds. In 2017 International \\nConference on 3D Vision (3DV), 537–547 (2017).\\n 26. Lawin, F .\\xa0 J. et al. Deep projective 3d semantic segmentation. In Computer Analysis of Images and Patterns: 17th International \\nConference, CAIP 2017, 95–107 (2017).\\n 27. Boulch, A. et al. Unstructured point cloud semantic labeling using deep segmentation networks. Eurographics 3, 17–24 (2017).\\n 28. Tatarchenko, M., Park, J., Koltun, V . & Zhou, Q.-Y . Tangent convolutions for dense prediction in 3d. In 2018 IEEE/CVF Conference \\non Computer Vision and Pattern Recognition (CVPR), 3887–3896 (2018).\\n 29. Wu, B., Wan, A., Yue, X. & Keutzer, K. Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object \\nsegmentation from 3d lidar point cloud. In 2018 IEEE International Conference on Robotics and Automation (ICRA), 1887–1893 \\n(2018).\\n 30. Rethage, D., Wald, J., Sturm, J., Navab, N. & Tombari, F . Fully-convolutional point networks for large-scale point clouds. In \\nProceedings of the European Conference on Computer Vision (ECCV), 596–611 (2018).\\n 31. Wu, B., Zhou, X., Zhao, S., Yue, X. & Keutzer, K. Squeezesegv2: Improved model structure and unsupervised domain adaptation \\nfor road-object segmentation from a lidar point cloud. In 2019 International Conference on Robotics and Automation (ICRA) , \\n4376–4382 (2019).\\n 32. Qi, C.\\xa0R., Su, H., Mo, K. & Guibas, L.\\xa0J. Pointnet: Deep learning on point sets for 3d classification and segmentation. In 2017 IEEE \\nConference on Computer Vision and Pattern Recognition (CVPR), 77–85 (2017).\\n 33. Thomas, H. et al. Kpconv: Flexible and deformable convolution for point clouds. In 2019 IEEE/CVF International Conference on \\nComputer Vision (ICCV), 6410–6419 (2019).\\n 34. Y an, X., Zheng, C., Li, Z., Wang, S. & Cui, S. Pointasnl: Robust point clouds processing using nonlocal neural networks with \\nadaptive sampling. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 5588–5597 (2020).\\n 35. Ronneberger, O., Fischer, P . & Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Lecture Notes in \\nComputer Science, Medical Image Computing and Computer-Assisted Intervention - MICCAI Vol. 2015, 234–241 (2015).\\n 36. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and \\nPattern Recognition (CVPR), 770–778 (2016).\\n 37. Redmon, J., Divvala, S., Girshick, R. & Farhadi, A. Y ou only look once: Unified, real-time object detection. In 2016 IEEE Conference \\non Computer Vision and Pattern Recognition (CVPR), 779–788 (2016).\\n 38. Ren, S., He, K., Girshick, R. & Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Trans. \\nPattern Anal. Mach. Intell. 39, 1137–1149 (2017).\\n 39. Liu, Z. et al. A convnet for the 2020s. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 11966–\\n11976 (2022).\\n 40. Ando, A. et al. Rangevit: Towards vision transformers for 3d semantic segmentation in autonomous driving. In Proceedings of the \\nIEEE/CVF conference on computer vision and pattern recognition, 5240–5250 (2023).\\n 41. Kong, L. et al.  Rethinking range view representation for lidar segmentation. In Proceedings of the IEEE/CVF International \\nConference on Computer Vision, 228–240 (2023).\\n 42. Kweon, H., Kim, J. & Y oon, K.-J. Weakly supervised point cloud semantic segmentation via artificial oracle. In Proceedings of the \\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 3721–3731 (2024).\\nScientific Reports |        (2024) 14:28513 13| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='43. Peng, B. et al. Oa-cnns: Omni-adaptive sparse cnns for 3d semantic segmentation. In Proceedings of the IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition, 21305–21315 (2024).\\n 44. Graham, B., Engelcke, M. & Van Der Maaten, L. 3d semantic segmentation with submanifold sparse convolutional networks. In \\n2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 9224–9232 (2018).\\n 45. Choy, C., Gwak, J. & Savarese, S. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In 2019 IEEE/CVF \\nConference on Computer Vision and Pattern Recognition (CVPR), 3070–3079 (2019).\\n 46. Wang, P .-S., Liu, Y ., Guo, Y .-X., Sun, C.-Y . & Tong, X. O-cnn: Octree-based convolutional neural networks for 3d shape analysis. \\nACM Trans. Graph. 36, 1–11 (2017).\\n 47. Riegler, G., Osman Ulusoy, A. & Geiger, A. Octnet: Learning deep 3d representations at high resolutions. In 2017 IEEE Conference \\non Computer Vision and Pattern Recognition (CVPR), 6620–6629 (2017).\\n 48. Klokov, R. & Lempitsky, V . Escape from cells: Deep kd-networks for the recognition of 3d point cloud models. In 2017 IEEE \\nInternational Conference on Computer Vision (ICCV), 863–872 (2017).\\n 49. Tian, S. et al. Automatic classification and segmentation of teeth on 3d dental model using hierarchical deep learning networks. \\nIEEE Access 7, 84817–84828 (2019).\\n 50. Wang, Y . et al. Dynamic graph CNN for learning on point clouds. ACM Trans. Graph. 38, 1–12 (2019).\\n 51. Zhao, H., Jiang, L., Fu, C.-W . & Jia, J. Pointweb: Enhancing local neighborhood features for point cloud processing. In 2019 IEEE/\\nCVF Conference on Computer Vision and Pattern Recognition (CVPR), 5560–5568 (2019).\\n 52. Ma, X., Qin, C., Y ou, H., Ran, H. & Fu, Y . Rethinking network design and local geometry in point cloud: A simple residual mlp \\nframework. arXiv preprint arXiv:2202.07123 (2022).\\n 53. Hao, J. et al. Toward clinically applicable 3-dimensional tooth segmentation via deep learning. J. Dent. Res. 101, 304–311 (2022).\\n 54. Deng, X., Zhang, W ., Ding, Q. & Zhang, X. Pointvector: a vector representation in point cloud analysis. In Proceedings of the IEEE/\\nCVF Conference on Computer Vision and Pattern Recognition, 9455–9465 (2023).\\n 55. Wu, X., Lao, Y ., Jiang, L., Liu, X. & Zhao, H. Point transformer v2: Grouped vector attention and partition-based pooling. Adv. \\nNeural. Inf. Process. Syst. 35, 33330–33342 (2022).\\n 56. Wu, X. et al. Point transformer v3: Simpler faster stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and \\nPattern Recognition, 4840–4851 (2024).\\n 57. He, Y . et al. Full point encoding for local feature aggregation in 3-d point clouds. IEEE Transactions on Neural Networks and \\nLearning Systems (2024).\\n 58. Duan, L. et al. Condaformer: Disassembled transformer with local structure enhancement for 3d point cloud understanding. Adv. \\nNeural Inf. Process. Syst. 36 (2024).\\n 59. Lai, X., Chen, Y ., Lu, F ., Liu, J. & Jia, J. Spherical transformer for lidar-based 3d recognition. In Proceedings of the IEEE/CVF \\nConference on Computer Vision and Pattern Recognition, 17545–17555 (2023).\\n 60. Kolodiazhnyi, M., Vorontsova, A., Konushin, A. & Rukhovich, D. Oneformer3d: One transformer for unified point cloud \\nsegmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 20943–20953 (2024).\\n 61. Lian, C. et al. Deep multi-scale mesh feature learning for automated labeling of raw dental surfaces from 3d intraoral scanners. \\nIEEE Trans. Med. Imaging 39, 2440–2450 (2020).\\n 62. Zhang, L. et al. Tsgcnet: Discriminative geometric feature learning with two-stream graph convolutional network for 3d dental \\nmodel segmentation. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 6695–6704 (2021).\\n 63. Zanjani, F . G. et al. Mask-mcnet: Tooth instance segmentation in 3d point clouds of intra-oral scans. Neurocomputing 453, 286–298 \\n(2021).\\n 64. Zheng, Y ., Chen, B., Shen, Y . & Shen, K. Teethgnn: semantic 3d teeth segmentation with graph neural networks. IEEE Trans. Visual \\nComput. Graph. 29, 3158–3168 (2022).\\n 65. Wu, T.-H. et al. Two-stage mesh deep learning for automated tooth segmentation and landmark localization on 3d intraoral scans. \\nIEEE Trans. Med. Imaging 41, 3158–3166. https://doi.org/10.1109/tmi.2022.3180343 (2022).\\n 66. Li, Z., Liu, T., Wang, J., Zhang, C. & Jia, X. Multi-scale bidirectional enhancement network for 3d dental model segmentation. In \\n2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), 1–5 (IEEE, 2022).\\n 67. Wang, X. et al. Convolutional neural network for automated tooth segmentation on intraoral scans. BMC Oral Health 24, 804 \\n(2024).\\n 68. Qiu, L. et al. Darch: Dental arch prior-assisted 3d tooth instance segmentation with weak annotations. In Proceedings of the IEEE/\\nCVF Conference on Computer Vision and Pattern Recognition, 20752–20761 (2022).\\n 69. Wang, H. et al. Weakly supervised tooth instance segmentation on 3d dental models with multi-label learning. In International \\nConference on Medical Image Computing and Computer-Assisted Intervention, 723–733 (Springer, 2024).\\n 70. Almalki, A. & Latecki, L.\\xa0J. Self-supervised learning with masked autoencoders for teeth segmentation from intra-oral 3d scans. In \\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 7820–7830 (2024).\\n 71. Bruna, J., Zaremba, W ., Szlam, A. & LeCun, Y . Spectral networks and locally connected networks on graphs. arXiv preprint \\narXiv:1312.6203 (2013).\\n 72. Boykov, Y ., Veksler, O. & Zabih, R. Fast approximate energy minimization via graph cuts. In Proceedings of the Seventh IEEE \\nInternational Conference on Computer Vision, 377–384 (1999).\\n 73. Wu, T.-H. et al.Machine (Deep) Learning for Orthodontic CAD/CAM Technologies, 117–129 (2021).\\nAuthor contributions\\nY .W ., H.Y . and K.D. conceptualized the work. Y .W . performed experiments and analysed the results together \\nwith H.Y . and K.D.  Y .W . wrote the first manuscript and revised by H.Y . and K.D. H.Y . and K.D. supervised the \\nproject and provided full guidance throughout the process. All authors reviewed the manuscript.\\nDeclarations\\nCompeting interests\\nThe authors declare no competing interests.\\nAdditional information\\nCorrespondence and requests for materials should be addressed to H.Y .\\nReprints and permissions information is available at www.nature.com/reprints.\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \\ninstitutional affiliations.\\nScientific Reports |        (2024) 14:28513 14| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2024-11-18T13:55:51+01:00', 'crossmarkdomains[1]': 'springer.com', 'moddate': '2024-11-18T16:22:11+01:00', 'trapped': '/False', 'crossmarkmajorversiondate': '2010-04-23', 'doi': '10.1038/s41598-024-79485-x', 'crossmarkdomains[2]': 'springerlink.com', 'crossmarkdomainexclusive': 'true', 'source': 'C:\\\\Users\\\\Moorthy\\\\Downloads\\\\Transformer based 3D tooth.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \\n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \\nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \\na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \\npermission under this licence to share adapted material derived from this article or parts of it. The images or \\nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \\notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \\nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \\nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \\nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \\n© The Author(s) 2024 \\nScientific Reports |        (2024) 14:28513 15| https://doi.org/10.1038/s41598-024-79485-x\\nwww.nature.com/scientificreports/')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04b8ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyArSlrZAf0klu58eAWKytamA5X0zwpS_oQ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef59f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Set up Gemini Embeddings\n",
    "# pip install sentence-transformers\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# \"all-MiniLM-L6-v2\" huffing face\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\",\n",
    "                                          google_api_key = gemini_key )\n",
    "# Create a FAISS vector store from your PDF chunks\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52cd2565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1a7e190ae30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab91b47",
   "metadata": {},
   "source": [
    "#### Set Up the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b422140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash-001\",\n",
    "                             api_key=gemini_key, temperature=0)\n",
    "# Create retriever from vector space\n",
    "retriever  = vectorstore.as_retriever()\n",
    "\n",
    "#Build the retrievalQA RAG chain\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a6d931a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How many types of classes are there in segmentation?\n",
      "A: Based on the context, the variable C represents the number of semantic labels, which can be interpreted as the number of classes in segmentation. However, the exact value of C is not provided in the context.\n"
     ]
    }
   ],
   "source": [
    "## Ask Questionsabout your PDF\n",
    "\n",
    "Question = \"How many types of classes are there in segmentation?\"\n",
    "\n",
    "answer = qa_chain.invoke({\"query\": Question})\n",
    "print(\"Q:\", Question)\n",
    "print(\"A:\", answer['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
